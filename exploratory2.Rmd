---
title: "Yelp: Exploratory Analysis"
author: "Ryan Kobler"
date: "11/18/2019"
output: pdf_document
---
### Packages:
```{r}
library(jsonlite)
library(tidytext)
library(stringr)
library(wordcloud)
library(dplyr)
library(ggplot2)
library(ggpairs)
```

### Load in the data:
```{r}
#path <- "/Users/ryankobler/Downloads/yelp_review.csv"
#yelp <- read.csv(path)

# Take sample of the data
#yelp %>% sample_frac(0.01)
# Save sample for easy access
#write.csv(df, "yelp-train.csv")
#yelp_review <- read.csv("yelp_review.csv")

# Take sample of the data
#yelp_sample <- yelp_review %>% sample_frac(0.01)
# Save sample for easy access
#write.csv(yelp_sample, "yelp-train.csv")

# Load training data
yelp_train <- read.csv("yelp-train.csv")
yelp_sample <- yelp_train
```

### Count total number of words
```{r}
#count the number of words in the
yelp_sample <- yelp_train
yelp_sample <- mutate(yelp_sample, numwords = str_count(yelp_sample$text, " "))
#univariate analysis of size
ggplot(data = yelp_sample, aes(x = numwords)) + geom_bar()
#analysis of size vs star rating with locally weighted polynomial
ggplot(data = yelp_sample, aes(x = numwords, y = stars)) + geom_jitter(size = 0.25) + geom_smooth()

```

### Extracting features:
```{r}
# Note: this function requires the tidytext package & drops all
# words that do not convey sentiment
dropStopwords <- function(string){
  # Remove all punctuation except apostrophes & replace with " "
  noPunc <- gsub("[^[:alnum:][:space:]']", " ", string) 
  # Split the larger string by space using strsplit()
  splitBySpace <- unlist(strsplit(noPunc, split = " "))
  # Remove missing chunks
  splitBySpace <- splitBySpace[splitBySpace != ""]
  todrop <- get_stopwords() # query dictionary of stopwords
  todrop <- todrop[[1]]
  
  # remove stop words and wrap as lowercase
  tolower(splitBySpace[!splitBySpace %in% todrop])
  
}
# Remove the stop words and save in new column
# yelp_train$prunedtext <- lapply(yelp_train$text, FUN = dropStopwords)


nrc <- get_sentiments("nrc")
get_sentiments("afinn")
get_sentiments("loughran")

```

### Testing on tiny data set
```{r}
yelp.small <- yelp_train[1:3,]
yelp.small$prunedtext <- lapply(yelp.small$text, FUN = dropStopwords)

yelp.small$prunedtext

# Matching sentiments from the NRC dictionary
nrc$sentiment <- as.factor(nrc$sentiment)

# takes sentiment string in nrc and a sequence of pruned words
# associated with 1 review
nrcSentimentCount <- function(senti, text){
  sentiment <- nrc %>% 
  filter(sentiment == senti) %>%
  select(word)
  
  # outputs a count of the number of "trues"
  sum(unlist(text) %in% unlist(sentiment))
}

yelp.small %>% 
  mutate(joy = nrcSentimentCount("fear", prunedtext))

# Grab all of the sentiments from for testing
sentiments <- levels(as.factor(nrc$sentiment))

# Generate new columns that count the number of times each sentiment word appears
yelp.small$joy <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "joy")
yelp.small$anger <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "anger")
yelp.small$fear <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "fear")
yelp.small$positive <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "positive")
yelp.small$negative <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "negative")
yelp.small$surprise <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "surprise")
yelp.small$disgust <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "disgust")
yelp.small$trust <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "trust")
yelp.small$anticipation <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "anticipation")
yelp.small$anger <- lapply(yelp.small$prunedtext, nrcSentimentCount, senti = "anger")
```


### Applying to the full training data set
```{r}
# Generate new column called prunedtext
yelp_train$prunedtext <- lapply(yelp_train$text, FUN = dropStopwords)

yelp_train$joy <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "joy")
yelp_train$anger <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "anger")
yelp_train$fear <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "fear")
yelp_train$positive <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "positive")
yelp_train$negative <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "negative")
yelp_train$surprise <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "surprise")
yelp_train$disgust <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "disgust")
yelp_train$trust <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "trust")
yelp_train$anticipation <- lapply(yelp_train$prunedtext, nrcSentimentCount, senti = "anticipation")
```


### Convert columns to numbers and normalize by length of review
```{r}
yelp_train$joy <- as.numeric(yelp_train$joy)/(yelp_train$nwords)
yelp_train$anger <-  as.numeric(yelp_train$anger)/(yelp_train$nwords)
yelp_train$fear <-  as.numeric(yelp_train$fear)/(yelp_train$nwords)
yelp_train$positive <-  as.numeric(yelp_train$positive)/(yelp_train$nwords)
yelp_train$negative <-  as.numeric(yelp_train$negative)/(yelp_train$nwords)
yelp_train$surprise <-  as.numeric(yelp_train$surprise)/(yelp_train$nwords)
yelp_train$disgust <-  as.numeric(yelp_train$disgust)/(yelp_train$nwords)
yelp_train$trust <-  as.numeric(yelp_train$trust)/(yelp_train$nwords)
yelp_train$anticipation <-  as.numeric(yelp_train$anticipation)/(yelp_train$nwords)
yelp_train$anticipation <-  as.numeric(yelp_train$anticipation)/(yelp_train$nwords)
```


```{r}
write.csv(yelp_train[, -12], "yelp-train3.csv")
```

```{r}
glimpse(yelp_train)
```



### Add the number of words (from Jacob)
```{r}
yelp_train$nwords <- str_count(yelp_train$text, " ")
```



### Generate word clouds for each rating level
```{r}
yelp_train$prunedtext <- lapply(yelp_train$text, FUN = dropStopwords)

words4 <- yelp_train %>%
  filter(stars == 4) %>%
  pull(prunedtext)

words5 <- yelp_train %>%
  filter(stars == 5) %>%
  pull(prunedtext)

words1 <- yelp_train %>%
  filter(stars == 1) %>%
  pull(prunedtext)

wordcloud(unlist(words5), min.freq = 10, max.words = 30)

wordcloud(unlist(words1), min.freq = 10, max.words = 30)
```

### Plot sentiments
Number of 'joy' sentiment words versus (standardized) number of 'anticipation' words, where the stars are categorical/colored accordingly. 

Bar chart of ratio of 'joy' to total number of words. 
```{r}

yelp_train$starsfactor <- as.factor(yelp_train$stars)
for(sent in sentiments){
  ratio <- paste0(sent, "ratio")
}
yelp_train$joyratio <- yelp_train$joy / yelp_train$nwords

yelp_train$angerratio <- yelp_train$anger / yelp_train$nwords
yelp_train$angerratio <- yelp_train$anger / yelp_train$nwords
yelp_train$angerratio <- yelp_train$anger / yelp_train$nwords
yelp_train$angerratio <- yelp_train$anger / yelp_train$nwords




ggplot(yelp_train, aes(starsfactor, joyratio)) + 
  geom_bar(stat = "identity")

ggplot(yelp_train, aes(starsfactor, angerratio)) + 
  geom_bar(stat = "identity")

ggplot(yelp_train, aes(starsfactor, joyratio)) + 
  geom_bar(stat = "identity")

```




### Data Description

The training data set, which is a 1% random sample of the entire yelp_review universe, is made up of 52,616 observations and 19 variables. The useful predictors include 
- `joy`: number of joy-categorized words that appear in the body of the review (after removing stop words). 

The sentiment levels categorized by the NRC lexicon are anger, anticipation, disgust, fear, negative, positive, sadness, surprise, and trust. And each of our variables named in this way follow the example given above.

We also include ratios for each, so `joyratio` is the total number of joy words divided by the total number of non-stop words. 

```{r}
# Glimpse the data set to examine the predictors
glimpse(yelp_train)

```







### Missingness:

```{r}

```





### Univariate analysis of the response

```{r}
mean(yelp_train$stars)
var(yelp_train$stars)
ggplot(yelp_train, aes(x=stars)) + 
  geom_histogram(binwidth=1) + 
  ggtitle("Bar Graph of Stars") 

```

The mean number of stars in our sample is around 3.726 and the variance is around 2.067. 

```{r}
ggplot(yelp_train, aes(x=positive, y = anger)) + 
  geom_point(aes(color = stars)) + 
  ggtitle("Scatter of Stars") 
```




### Bivariate/Trivariate Graphs

```{r}
bool <- sapply(yelp_train, is.numeric)
num_only <- yelp_train[,bool]
#scatterplot matrix of all of the variables
ggpairs(num_only, labels = colnames(num_only)) # this is not very useful
#so I create a data.frame with only 4 essential variables
selected <- data.frame(num_only$nwords, num_only$positive, num_only$negative, num_only$anger)
#do a corrgram on those vars
ggpairs(selected, labels = colnames(selected))
ggplot(aes(geom_point))
#selected scatterplots of important variables
ggplot(data = num_only, aes(x = nwords, y = stars)) + geom_jitter(size = 0.25) + geom_smooth() #stars
ggplot(data = num_only, aes(x = nwords, y = positive)) + geom_point(aes(color = stars))
ggplot(data = num_only, aes(x = nwords, y = negative)) + geom_point(aes(color = stars))
#adding a column that is positive - negative
num_only$netpos <- num_only$positive - num_only$negative
ggplot(data = num_only, aes(x = nwords, y = netpos)) + geom_point(aes(color = stars))
ggplot(data = num_only, aes(x = positive, y = negative)) + geom_point(aes(color = stars))
cor(num_only, use = "complete.obs")
#on the smaller set
cor(selected, use = "complete.obs")


```













```










































