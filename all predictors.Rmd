---
title: "Start to finish-data frame"
author: "Ryan Kobler"
date: "12/2/2019"
output: pdf_document
---

## Load packages
```{r}
library(dplyr)
library(tokenizers)
library(tidytext)
library(tidyr)
library(stringr)
```

Define functions
```{r}
# Note: this function requires the tidytext package & drops all
# words that do not convey sentiment
dropStopwords <- function(string){
  # Remove all punctuation except apostrophes & replace with " "
  noPunc <- gsub("[^[:alnum:][:space:]']", " ", string) 
  # Split the larger string by space using strsplit()
  splitBySpace <- unlist(strsplit(noPunc, split = " "))
  # Remove missing chunks
  splitBySpace <- splitBySpace[splitBySpace != ""]
  todrop <- get_stopwords() # query dictionary of stopwords
  todrop <- todrop[[1]]
  
  # remove stop words and wrap as lowercase
  tolower(splitBySpace[!splitBySpace %in% todrop])
  
}

# Function below takes sentiment string in nrc and a sequence of 
# pruned words associated with 1 review
nrcSentimentCount <- function(senti, text){
  sentiment <- nrc %>% 
  filter(sentiment == senti) %>%
  select(word)
  
  # outputs a count of the number of "trues"
  sum(unlist(text) %in% unlist(sentiment))
}

# Remove the stop words and save in new column
#yelp_train$prunedtext <- lapply(yelp_train$text, FUN = dropStopwords)

```


## Bring in withlanguage data

```{r}
yelp_train <- read.csv("DATA/withlanguage.csv")

yelp_train <- yelp_train %>%
  filter(language == "english" | language == "scots")
```


## Adding punctuation predictors

```{r}
yelp_train$starsfactor <- as.factor(yelp_train$stars)

nSentences <- function(string){
  length(tokenize_sentences(string))
}

# Use tokenize_sentences() function to separate the long body of text into
# sentence segments. 
sentences <- tokenize_sentences(as.character(yelp_train$text))
yelp_train$sentences <- sapply(sentences, length)

#-----------------------------------------------------
# Punctuation mark frequency

# Use the stringr library to count the number of punctuation marks of all
# types, designated by the "[[:punct:]]" pattern and save in new var `punct`.
yelp_train$punct <- str_count(as.character(yelp_train$text),
                              "[[:punct:]]")

yelp_train$exclaim <- str_count(as.character(yelp_train$text),
                              "!")
```


## Adding negation words using the Afinn lexicon

```{r}


negations <- c("not", "no", "never", "without")

bigrams <- yelp_train %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# separate the 2-word bigrams into columns word1, word2
bigrams_sep <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# How do we categorize the negated bigrams within our preexisting
# sentiment categories: "anger" "joy" etc.?

# I will use a different lexicon that weights sentiments numerically
# to simply flip the sign on negated sentiment words. However, the ratios
# are still capturing the good in "not good" rather than discounting them.

# Bring in afinn sentiments
afinn <- get_sentiments("afinn")

# ----------------------------------------

afinn_scores <- bigrams_sep %>%
  inner_join(afinn, by = c(word2 = "word")) %>%
  mutate(ifelse(word1 %in% negations,
                -value, 
                value)) %>%
  group_by(X) %>%
  summarise(score = mean(value))


# JOIN THE AFINN SCORES TO YELP_TRAIN df
yelp_train <- left_join(yelp_train, afinn_scores, by = "X")
```


## Bring in neutral sentiment words
We realized that the model was predicting the tail star ratings well, which may be a result of the chosen sentiment predictors. For instance, we don't have a "mediocre" lexicon, but do capture mostly the extreme sentiments: anger, joy, disgust. 

```{r}
vader <- read.delim("DATA/vader_lexicon.txt")
vader$word <- vader$X..
vader$rating <- vader$X.1.5
vader$sd <- vader$X0.80623

# Arbitrarily choose neutral thresholds less than 1.5 in intvl [-3, 3]
neu.df <- vader %>% 
  filter(abs(rating) < 1.5 & sd < 1) %>%
  select(word, rating, sd)

# input should be the `prunedtext` column
countNeutrals <- function(text){
  sum(unlist(text) %in% as.character(neu.df$word))
}

yelp_train$neutral <- sapply(yelp_train$prunedtext, countNeutrals)
yelp_train$neutralratio <- yelp_train$neutral / yelp_train$nwords

onestar <- yelp_train %>% filter(stars == 1)
twostar <- yelp_train %>% filter(stars == 2)
threestar <- yelp_train %>% filter(stars == 3)
fourstar <- yelp_train %>% filter(stars == 4)
fivestar <- yelp_train %>% filter(stars == 5)

summary(onestar$neutralratio) 
summary(twostar$neutralratio)
summary(threestar$neutralratio)
summary(fourstar$neutralratio)
summary(fivestar$neutralratio)



```

## 

## Write to csv
```{r}
write.csv(yelp_train, "DATA/training_newpred.csv")
```


